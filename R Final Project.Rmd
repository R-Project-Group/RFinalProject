---
title: "Final Project"
author: "Phillip Ross, Ming Chen, Hersh Agarwal"
date: "5/6/2021"
output: html_document
---

# What are the drivers of police killings in the US?

## Executive Summary

## Goal of the study
Police killings have been on the forefront of the news in the recent years and is a prominent issue the United States is facing today. Our objective in this study is to identify insights within this dataset that may illuminate drivers of killings and identify possible biases in how police use lethality in their techniques.

## Data sources
The primary data-set our study is based on is a police killings database maintained by the Washington Post. The data is comprehensive starting from January 1st, 2015 through early May 2021, compiled from local news reports, law enforcement websites, and mining independent databases dedicated to tracking police killings. See the source article here: https://www.washingtonpost.com/graphics/investigations/police-shootings-database/ and the available data here: https://github.com/washingtonpost/data-police-shootings

To supplement this data, we utilized the Census Bureau data tool to download demographic and economic information available at the county level and joined it with the killings data by county (or FIPS code). The Census Bureau has a data query tool that appears fairly comprehensive but is also a bit un-intuitive to use. Nonetheless, we pulled in five different county-level data-sets from the American Community Survey which was last taken in 2019. These reports include 1) Demographics and Housing, Educational Attainment, Financial Characteristics, Selected Economics Characteristics, Selected Housing Characteristics. 

#to do - list what we did. 

#conclusion

## Findings

#regression / lasso
#data vis, counties

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require('pacman')) {install.packages('pacman')}
pacman::p_load(tidyverse, stringr, dplyr, ggplot2, ggthemes, data.table, lubridate,
               RColorBrewer, ggsci, plotROC, usmap,
               plotly, ggpubr, vistime, RCurl, RJSONIO, sf, tigris, devtools, gtsummary, cli)
library(dplyr)
```

## R Markdown

##EDA / Data Preparation

```{r}
# Read-in data

# These are census shape files which we need to map latitude and longitutde to FIPs codes
census_tracts <- st_read("CensusShapes2/cb_2018_us_county_within_cd116_500k.shp", quiet = TRUE)

# This is the police killing data from Kaggle
killdata <- read.csv("PoliceKillingsUS_MC.csv")

# This is a look-up table that maps FIPS code to County name and State (from: https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697)
fipsLookUp <- read.csv("FIPSCountyStateLookUp.csv", colClasses ='character') 
fipsLookUp$fips <- fipsLookUp[,1] # Import has made the FIPS column name weird so re-making column

# First-off, we need take our geo-coordinates of each police killing and map it to a county (uniquely identified by a FIPS code). This is because all the census data we plan to join in is readily available at the county level

# There are APIs that do this, but we couldn't figure out how to get them to work. Instead, we found a tutorial that relies on plotting lat/long within county boundaries as defined in the shape files. Tutorial here: https://shiandy.com/post/2020/11/02/mapping-lat-long-to-fips/

# Remove the lat/long from the killing data to feed into this algorithm
killdata_latlong <- data.frame(latitude = killdata$latitude, longitude = killdata$longitude)

# Remove observations without lat/long and convert them into a shape file
latlong_sf <- killdata_latlong %>%
  filter(!is.na(latitude), !is.na(longitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = st_crs(census_tracts))

# Check the overlap between the lat/long and the census tracts
intersected <- st_intersects(latlong_sf, census_tracts) 

# Indicate overlap and if there is overlap, bring in the GEOID
latlong_final <- latlong_sf %>%
  mutate(intersection = as.integer(intersected),
         fips = if_else(is.na(intersection), "",
                        census_tracts$GEOID[intersection]))

# GEOID does not seem to be FIPS exactly -- pull the first two and last three characters to get to the FIPS
latlong_final$mod_fips <- paste(str_sub(latlong_final$fips, 1, 2), str_sub(latlong_final$fips, -3, -1), sep = "") 

# We remove all killings that don't have a latitude/longitude. We go from 6268 observations to 5963 observations which is not a terrible fall-out.
killdata_clean <- killdata %>%
  filter(!is.na(latitude), !is.na(longitude)) # Remove all killings that don't have latitude/longitude

# We take the FIPs code that we have generated from the lat/long and append it onto the data
killdata_clean$mod_fips <- latlong_final$mod_fips  
```

```{r}
# Read-in census data. All data was sourced from the American Community Survey in 2019 and pulled at the FIPS level for all counties

# Skip the first row, as it is a redundant header row
demoData <- read.csv("Data (County-Level)/Demographics and Housing/ACSDP1Y2019.DP05_data_with_overlays_2021-04-29T104649.csv", skip = 1, header = T)
# ACS DEMOGRAPHIC AND HOUSING ESTIMATES

eduData <- read.csv("Data (County-Level)/Educational Attainment/ACSST1Y2019.S1501_data_with_overlays_2021-05-07T192056.csv", skip = 1, header = T)
# ACS EDUCATIONAL ATTAINMENT

financeData <- read.csv("Data (County-Level)/Financial Characteristics/ACSST1Y2019.S2503_data_with_overlays_2021-05-07T193752.csv", skip = 1, header = T)
# ACS FINANCIAL CHARACTERISTICS

econData <- read.csv("Data (County-Level)/Selected Economic Characteristics/ACSDP1Y2019.DP03_data_with_overlays_2021-05-07T193216.csv", skip = 1, header = T)
# ACS SELECTED ECONOMIC CHARACTERISTICS

housingData <- read.csv("Data (County-Level)/Selected Housing Characteristics/ACSDP1Y2019.DP04_data_with_overlays_2021-04-29T141348.csv", skip = 1, header = T)
# ACS SELECTED HOUSING CHARACTERISTICS
```

```{r}
# Join together all the demo data by ID (in this case the FIPS)
masterCensus <- demoData %>%
  left_join(econData, by="id") %>%
  left_join(eduData, by="id") %>%
  left_join(housingData, by="id") %>%
  left_join(financeData, by="id")

# Half of the columns are columns which estimate how much error they may have been in the measurement. These are a bit too complex to work with, so let's just remove them
# There are also overlapping columns across each set. These get a '.y'. or '.x' added to them so can be identified and removed.  
masterCensus_s2 <- masterCensus %>%
  select(-contains("Error")) %>% # Removes columns that indicator the error
  select(-contains(".y")) %>% # Indicates a duplicated variable
  select(-contains(".x"))
```

```{r}
# Let's look at the killings data to understand what data we have, and what sort of dependent variable we want to create. 

killdata_clean %>%
  group_by(manner_of_death, signs_of_mental_illness, threat_level, flee, body_camera) %>%
  summarise(count = n())

killdata_clean %>%
  group_by(manner_of_death) %>%
  summarise(count = n()) # Two options: 'shot' and 'shot and Tasered' (low freq.) can roll-up

killdata_clean %>%
  group_by(signs_of_mental_illness) %>%
  summarise(count = n()) # Roughly 25% of victims had signs of mental illness (Y or N)

killdata_clean %>%
  group_by(threat_level) %>%
  summarise(count = n()) # Three categories, 'attack', 'other' and 'undetermined' (low freq)

killdata_clean %>%
  group_by(flee) %>%
  summarise(count = n()) # Five options: a blank, 'Car', 'Foot', 'Not fleeing' and 'Other'; potentially roll-up to fleeing and not fleeing

killdata_clean %>%
  group_by(body_camera) %>%
  summarise(count = n()) # Options FALSE or TRUE. Only ~14% of deaths had body cameras

# Given that there is not much data is it is -- it may not make sense to differentiate different types of killings (e.g. those where there was a threat or signs of mental illness). It may not make much sense to differentiate by year either. 
```

```{r}
# Curious to see how the killing is disseminated across counties represented in the data. Below is code to construct a pareto chart of killings by FIPS. 

killdata_FIPS <- killdata_clean %>% # Create a county-level data-set with # of killings
  group_by(mod_fips) %>%
  summarise(totalKillings = n()) %>%
  rename(fips = mod_fips)

paretoData <- arrange(killdata_FIPS, desc(totalKillings)) %>%
  mutate(cumsum = cumsum(totalKillings),
         freq = round(totalKillings/sum(totalKillings), 3), 
         cum_freq = cumsum(freq))

## Saving Parameters 
def_par <- par()

# New margins
par(mar=c(5,5,4,5))

## plot bars, pc will hold x values for bars
pc = barplot(paretoData$totalKillings,  
             width = 1, space = 0.2, border = NA, axes = F,
             ylim = c(0, 1.05 * max(paretoData$totalKillings, na.rm = T)), 
             ylab = "Cumulative Counts" , cex.names = 0.7, 
             names.arg = paretoData$fips,
             main = "Pareto Chart (version 2)")

## anotate left axis
axis(side = 2, at = c(0, paretoData$totalKillings), las = 1, col.axis = "grey62", col = "grey62", tick = T, cex.axis = 0.8)

## frame plot
box( col = "grey62")

## Cumulative Frequency Lines 
lines(pc, paretoData$cumsum, type = "b", cex = 0.7, pch = 19, col="cyan4")

## Annotate Right Axis
px <- paretoData$cum_freq * max(paretoData$totalKillings, na.rm = T)
lines(pc, px, type = "b", cex = 0.7, pch = 19, col="cyan4")

## restoring default parameter
axis(side = 4, at = c(0, px), labels = paste(c(0, round(paretoData$cum_freq * 100)) ,"%",sep=""), 
     las = 1, col.axis = "grey62", col = "cyan4", cex.axis = 0.8, col.axis = "cyan4")

# One FIPs has a staggeringly high killing number (255), but we have to scale this by population. There is a very long right tail and from the pareto shape it does seem like certain counties are over-represented.

```

```{r}
# The maximum number of total killings is in Los Angeles County at 255. 
killdata_FIPS$fips[which.max(killdata_FIPS$totalKillings)]
killdata_FIPS$totalKillings[which.max(killdata_FIPS$totalKillings)]
```

```{r}
# install.packages('skimr')
library(skimr)
# summary(killdata_clean)

# Evaluate data across time
killdata_fips_year <- killdata_clean %>%
  mutate(date = as.Date(killdata_clean$date, format = "%m/%d/%Y")) %>%
  mutate(year = lubridate::year(date)) %>%
  group_by(year) %>%
  summarise(totalKillings = n())

# There is not much difference year over year; might not make sense to separate this by years

# Pull out the FIPS code out of the ID identified in the census data
masterCensus_s2$fips <- str_sub(masterCensus_s2$id, -5, -1)
```

```{r}
# Join the killings data to the census data. Create the killing rate response variable as well as transformed version because the rates are very low. 
sdl1 <- masterCensus_s2 %>%
    left_join(killdata_FIPS, by="fips") %>%
    mutate(killingRate =  totalKillings/Estimate..SEX.AND.AGE..Total.population) %>%
    mutate(killingRateEff = coalesce(killingRate, 0.000001)) %>%
    mutate(killingRateLog = log(killingRateEff)) %>%
    select(fips,totalKillings,killingRate,killingRateLog,everything())
```

```{r}
# We will do some variable/column cleaning

# Step 1 cleaning; only keep columns that have 'percentage' data. Countries differ greatly by size, so the nominal data could push results one way or another. By using percentage data, each variable in each county receives equal weight
sdl2 <- sdl1 %>%
    select(contains("Percent"))

# Step 2 cleaning; identify non-numeric values within census data
table(unlist(sdl2, use.names = FALSE)[(!grepl('^[0-9]',unlist(sdl2, use.names = FALSE)))])

# It looks like things are coded (X) or N if they are missing; let's look at which columns have a high percentage of (X) or N and are thus worth dropping

# Flag any column where more than half the values are '(X)'
library(plyr)
count.X.per.column <- ldply(sdl2, function(c) sum(c=="(X)"))
columnsToExclude1 <- count.X.per.column$.id[count.X.per.column$V1 > 420]

# Flag any column where more than half of the values are 'N'
count.N.per.column <- ldply(sdl2, function(c) sum(c=="N"))
columnsToExclude2 <- count.N.per.column$.id[count.N.per.column$V1 > 420]

# Drop columns with too many NULL values
sdl3 <- sdl2 %>%
  select(-columnsToExclude1)

sdl4 <- sdl3 %>%
  select(-columnsToExclude2)

# Convert remaining data frame into numeric columns (will force some values to NA)
sdl5 <- mutate_all(sdl4, function(x) as.numeric(as.character(x)))

# Flag any column where the minimum is above 100 (percentage cannot be over 100)
colMin <- apply(sdl5, 2, min, na.rm = TRUE)
columnsToExclude3 <- names(colMin)[colMin>100]

# Drop columns with values that are below the expected range 
sdl6 <- sdl5 %>%
  select(-columnsToExclude3)

# Flag any column where the maximum is above 100 (percentage cannot be over 100)
colMax <- apply(sdl6, 2, max, na.rm = TRUE)
columnsToExclude4 <- names(colMax)[colMax>100]

# Drop columns with value that exceeds the expected range
sdl7 <- sdl6 %>%
  select(-columnsToExclude4)

## NEW ADDITIONS ##
manualForceIn <- read.csv('census_columns_keep.csv.', header = FALSE)
manualForceIn[1,] <- "Estimate..SEX.AND.AGE..Total.population"

manualForceInCols <- intersect(names(sdl7), manualForceIn[,1])

sdl7 <- sdl7[, manualForceInCols]

# Re-add response variables back to cleaned census data
sdl7$fips <- sdl1$fips
sdl7$totalKillings <- sdl1$totalKillings
sdl7$killingRate <- sdl1$killingRate
sdl7$killingRateLog <- sdl1$killingRateLog

# Re-add key predictor variables which were excluded
sdl7$fips <- sdl1$fips
sdl7$Estimate..GROSS.RENT..Occupied.units.paying.rent..Median..dollars. <- sdl1$Estimate..GROSS.RENT..Occupied.units.paying.rent..Median..dollars.
# sdl7$Estimate..HOUSING.OCCUPANCY..Total.housing.units..Homeowner.vacancy.rate <- sdl1$Estimate..HOUSING.OCCUPANCY..Total.housing.units..Homeowner.vacancy.rate
# sdl7$Estimate..HOUSING.OCCUPANCY..Total.housing.units..Rental.vacancy.rate <- sdl1$Estimate..HOUSING.OCCUPANCY..Total.housing.units..Rental.vacancy.rate
sdl7$Estimate..HOUSING.TENURE..Occupied.housing.units..Average.household.size.of.owner.occupied.unit <- sdl1$Estimate..HOUSING.TENURE..Occupied.housing.units..Average.household.size.of.owner.occupied.unit
sdl7$Estimate..HOUSING.TENURE..Occupied.housing.units..Average.household.size.of.renter.occupied.unit <- sdl1$Estimate..HOUSING.TENURE..Occupied.housing.units..Average.household.size.of.renter.occupied.unit
sdl7$Estimate..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Per.capita.income..dollars. <- sdl1$Estimate..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Per.capita.income..dollars.
sdl7$Estimate..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households..Median.household.income..dollars. <- sdl1$Estimate..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households..Median.household.income..dollars.
sdl7$Estimate..ROOMS..Total.housing.units..Median.rooms <- sdl1$Estimate..ROOMS..Total.housing.units..Median.rooms
sdl7$Estimate..SELECTED.MONTHLY.OWNER.COSTS..SMOC...Housing.units.with.a.mortgage..Median..dollars. <- sdl1$Estimate..SELECTED.MONTHLY.OWNER.COSTS..SMOC...Housing.units.with.a.mortgage..Median..dollars.
sdl7$Estimate..SELECTED.MONTHLY.OWNER.COSTS..SMOC...Housing.units.without.a.mortgage..Median..dollars. <- sdl1$Estimate..SELECTED.MONTHLY.OWNER.COSTS..SMOC...Housing.units.without.a.mortgage..Median..dollars.
sdl7$Estimate..SEX.AND.AGE..Total.population <- sdl1$Estimate..SEX.AND.AGE..Total.population
sdl7$Estimate..VALUE..Owner.occupied.units..Median..dollars. <- sdl1$Estimate..VALUE..Owner.occupied.units..Median..dollars.

names(sdl7)
detach("package:plyr", unload = TRUE)
# Join on county and state variables onto data layer
sdl8 <- sdl7 %>%
  left_join(fipsLookUp, by="fips") %>%
  select(fips,County,State,totalKillings,killingRate,killingRateLog,everything()) %>%
  select(-ï..FIPS)
# options(max.print=400)
```

#Model Selection

```{r}

# Remove non-predictive variables
sdl8sub = sdl8[,-1:-5]

# Glmnet doesn't play with NAs, so coalesce all NAs into 0
sdl9sub <- sdl8sub %>%
   mutate_all(~replace(., is.na(.), 0)) 

set.seed(1)

# Create model matrix for GLM
X.fl <- model.matrix(killingRateLog~., data=sdl9sub)[,-1]
Y <- sdl9sub$killingRateLog

# Run CV glmnet with ten-folds
fit.lasso.0 <- glmnet::cv.glmnet(X.fl,Y, nfolds = 10, intercept =T)
plot(fit.lasso.0)

# Identify variables associated with MSE at 1SE above minimum
coef.1se <- coef(fit.lasso.0, s="lambda.1se")
var.1se <- coef.1se@Dimnames[[1]][coef.1se@i + 1][-1]

# Extract selected variables out of data-set
data.fl.sub <- sdl9sub[,c("killingRateLog", var.1se)] # get a 

# Re-name columns so that the regression output is more presentable
data.fl.sub2 <- data.fl.sub %>%
  rename(Pop_OneRace = Percent..RACE..Total.population..One.race) %>%
  rename(Pop_White = Percent..Race.alone.or.in.combination.with.one.or.more.other.races..Total.population..White) %>%
  rename(Pop_AmericanIndianAlaskan = Percent..Race.alone.or.in.combination.with.one.or.more.other.races..Total.population..American.Indian.and.Alaska.Native) %>%
  rename(Pop_HispanicLatino = Percent..HISPANIC.OR.LATINO.AND.RACE..Total.population..Hispanic.or.Latino..of.any.race.) %>%
  rename(Income_25to35k = Percent..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households...25.000.to..34.999) %>%
  rename(Income_50to75k = Percent..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households...50.000.to..74.999) %>%
  rename(Income_wSocialSec = Percent..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households..With.Social.Security) %>%
  rename(Income_wSuppSec = Percent..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households..With.Supplemental.Security.Income) %>%
  rename(Income_foodStamp = Percent..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households..With.Food.Stamp.SNAP.benefits.in.the.past.12.months) %>% 
  rename(HealthCoverage = Percent..HEALTH.INSURANCE.COVERAGE..Civilian.noninstitutionalized.population..With.health.insurance.coverage) %>%
  rename(PrivHealthCov = Percent..HEALTH.INSURANCE.COVERAGE..Civilian.noninstitutionalized.population..With.health.insurance.coverage..With.private.health.insurance) %>%
  rename(noHealthCov = Percent..HEALTH.INSURANCE.COVERAGE..Civilian.noninstitutionalized.population..No.health.insurance.coverage) %>%
  rename(edu_BlackHighSchool = Estimate..Percent..RACE.AND.HISPANIC.OR.LATINO.ORIGIN.BY.EDUCATIONAL.ATTAINMENT..Black.alone..High.school.graduate.or.higher) %>%
  rename(vacantHousing = Percent..HOUSING.OCCUPANCY..Total.housing.units..Vacant.housing.units) %>%
  rename(duplexes = Percent..UNITS.IN.STRUCTURE..Total.housing.units..2.units) %>%
  rename(mdus = Percent..UNITS.IN.STRUCTURE..Total.housing.units..5.to.9.units) %>%
  rename(mobileHomes = Percent..UNITS.IN.STRUCTURE..Total.housing.units..Mobile.home) %>%
  rename(twoVehiclesAvailable = Percent..VEHICLES.AVAILABLE..Occupied.housing.units..2.vehicles.available) %>%
  rename(noPlumbing = Percent..SELECTED.CHARACTERISTICS..Occupied.housing.units..Lacking.complete.plumbing.facilities) %>%
  rename(houseswMortages = Percent..MORTGAGE.STATUS..Owner.occupied.units..Housing.units.with.a.mortgage) %>%
  rename(medianRent = Estimate..GROSS.RENT..Occupied.units.paying.rent..Median..dollars.) %>%
  rename(roomsInHousing = Estimate..ROOMS..Total.housing.units..Median.rooms) %>%
  rename(ownerCostsHousing = Estimate..SELECTED.MONTHLY.OWNER.COSTS..SMOC...Housing.units.without.a.mortgage..Median..dollars.) %>%
  rename(totalPop = Estimate..SEX.AND.AGE..Total.population)

names(data.fl.sub)

# Use selected variables in Relaxed-LASSO
fit.min.lm <- lm(killingRateLog~., data=data.fl.sub2)

# We compare this against the model with the full set of variables
fit.all.lm <- lm(killingRateLog~., data=sdl9sub)

summary(fit.min.lm)
# summary(fit.all.lm)
# R^2 is only 0.3261

# Diagnostic plots
plot(fit.min.lm$fitted, fit.min.lm$residuals,
pch = 16,
main = "residual plot")
abline(h=0, lwd=4, col="red")

qqnorm(fit.min.lm$residuals)

library(gtsummary)

# Get summary of regression results
fit.min.lm %>%
    tbl_regression()
fit.min.lm
```



```{r}
# Runs a linear chart for killings each year
p <- ggplot(data=killdata_fips_year, aes(x=year, y=totalKillings)) +
    geom_line() +
    geom_point()
p + xlim(c(2015, 2020)) +ylim(c(900, 1000))
```

There does not seem to be any discernable pattern in killings from year to year. 

```{r}
# installs urbnmapr package
devtools::install_github("UrbanInstitute/urbnmapr")
```

```{r}
#creates a dataframe of all counties from urbanmapr
counties_sf <- urbnmapr::get_urbn_map(map = "counties", sf = TRUE)
head(counties_sf)
```

```{r}
#creates new data frame that groups killings by fips only
killdata_fips_county <-
  killdata_clean %>%
  mutate(date = as.Date(killdata_clean$date, format = "%m/%d/%Y")) %>%
  mutate(year = lubridate::year(date)) %>%
  group_by(mod_fips) %>%
  summarise(totalKillings = n()) %>%
  rename(fips = mod_fips)
```

```{r}
#joins that data frame with census data to get a killing rate
fulldata <- left_join(killdata_fips_county, masterCensus_s2, by="fips") %>%
  mutate(killingRate =  (totalKillings/Estimate..SEX.AND.AGE..Total.population)*100000/6) %>%
  mutate(killingRateEff = coalesce(killingRate, 0.000001)) %>%
  mutate(killingRateLog = log(killingRateEff)) %>%
  arrange(desc(killingRate)) %>%
  rename(county_fips = fips) %>%
  select(county_fips,totalKillings,killingRate,killingRateLog)
head(fulldata,10)
```
The 10 highest county annual killing rates over the six year period of 2015-2020 are: Tehama, CA, Pueblo, CO, Fairbanks North Star, AK, Shasta, CA, Muskogee, OK, St. Louis, MO, Lea, NM, Coconino, AZ, Apache, AZ, Yellowstone, MT. It's hard to discern commonalities among these ten counties, but one clear trend is that they are almost all in the Western part of the United States. None of them are in major cities, either. 

```{r}
#joins the data frame with FIPs and killing rate with the county map
county_map <- left_join(counties_sf, fulldata, by = "county_fips", copy = TRUE)

#filters to make all FIPS with zero killing 0
county_map_filtered <- 
  county_map %>%
  mutate_all(~replace(., is.na(.), 0))

#plots the map
county_map_filtered %>%
  ggplot() +
  geom_sf(mapping = aes(fill = killingRate),
          color = NA, size = 0.05) +
  labs(fill = "Annual Rate of Police Killings") + 
  scale_fill_distiller(palette = "Spectral")
```
It looks like generally, the more populous areas of the country have higher rates of killing (lots of Midwestern counties with zero killings). The southwest in particular seems to be an area with especially higher rates of killing. 


Analysis of black killings per capita vs. overall killings per capita
```{r}
killdata_raceanalysis <- killdata_clean %>%
  rename(fips = mod_fips) %>%
  filter(race == 'B') %>%
  group_by(fips) %>%
  summarise(totalBlackKillings = n()) %>%
  left_join(masterCensus_s2, by="fips") %>%
  mutate(blackPop = as.numeric(Percent..Race.alone.or.in.combination.with.one.or.more.other.races..Total.population..Black.or.African.American)/100*as.numeric(Estimate..SEX.AND.AGE..Total.population)) %>%
  na.omit() %>%
  mutate(blacksRatio = as.numeric(totalBlackKillings)/as.numeric(blackPop)*10000) %>%
  select(fips, totalBlackKillings, blacksRatio, blackPop)


killdata_FIPS2 <- killdata_FIPS %>%
  left_join(masterCensus_s2, by="fips") %>%
  na.omit() %>%
  mutate(overallRatio = as.numeric(totalKillings)/as.numeric(Estimate..SEX.AND.AGE..Total.population)*10000) %>%
  select(fips, totalKillings, overallRatio, Estimate..SEX.AND.AGE..Total.population,) %>%
  rename(overallPop = Estimate..SEX.AND.AGE..Total.population)


killdata_raceanalysis <- killdata_raceanalysis %>%
  left_join(killdata_FIPS2, by="fips") %>%
  mutate(ratioDiff = blacksRatio - overallRatio)

head(killdata_raceanalysis)

sum(killdata_clean$race == 'B', na.rm=TRUE)
sum(masterCensus_s2$Percent..Race.alone.or.in.combination.with.one.or.more.other.races..Total.population..Black.or.African.American)

```


```{r}

plot(x = killdata_raceanalysis$blacksRatio, 
     y = killdata_raceanalysis$overallRatio, 
     pch  = 16,     # "point character": shape/character of points 
     cex  = 0.8,    # size
     col  = "blue", # color 
     xlab = "Black Killings per 10K Pop",  # x-axis
     ylab = "Overall Killings per 10K Pop",  # y-axis 
     main = "Comparison of Killings/Population Ratio",
     xlim=c(0,3),
     ylim=c(0,1),
     abline(coef = c(0,1)))

plot(x = killdata_raceanalysis$blacksRatio, 
     y = killdata_raceanalysis$overallRatio, 
     pch  = 16,     # "point character": shape/character of points 
     cex  = 0.8,    # size
     col  = "blue", # color 
     xlab = "Black Killings per 10K Pop",  # x-axis
     ylab = "Overall Killings per 10K Pop",  # y-axis 
     main = "Comparison of Killings/Population Ratio",
     xlim=c(0,3),
     ylim=c(0,1),
     abline(lm(overallRatio ~ blacksRatio, data = killdata_raceanalysis), col = "blue"))
    
```
Blacks are killed at a significantly higher rate than the rest of the population on a per capita basis.



```{r}
#creates new data frame that groups killings by fips AND year
killdata_fips_yearcounty <-
  killdata_clean %>%
  mutate(date = as.Date(killdata_clean$date, format = "%m/%d/%Y")) %>%
  mutate(year = lubridate::year(date)) %>%
  group_by(mod_fips, year) %>%
  summarise(totalKillings = n()) %>%
  rename(fips = mod_fips)
```

```{r}
#joins that data frame with census data to get a killing rate
fulldata2 <- left_join(killdata_fips_yearcounty, masterCensus_s2, by="fips") %>%
  mutate(killingRate =  (totalKillings/Estimate..SEX.AND.AGE..Total.population)*100000) %>%
  mutate(killingRateEff = coalesce(killingRate, 0.000001)) %>%
  mutate(killingRateLog = log(killingRateEff)) %>%
  rename(county_fips = fips) %>%
  select(county_fips,totalKillings,killingRate, killingRateLog, year)
```


```{r}
county_map2 <- left_join(counties_sf, fulldata2, by = "county_fips", copy = TRUE)
county_map_filtered2 <- 
  county_map2 %>%
  filter(year < 2021 & year >2014) %>%
  mutate_all(~replace(., is.na(.), 0)) 


county_map_filtered2 %>%
  ggplot() +
  geom_sf(mapping = aes(fill = killingRate),
          color = NA, size = 0.05) +
  labs(fill = "Annual Rate of Police Killings") + 
  scale_fill_distiller(palette = "Spectral") +
  facet_wrap(~year)

```
```
It is again difficult to discern a particular pattern between years, but it does appear that in the majority of the years, the West has many of the counties with the highest rate of Police Killings. 
