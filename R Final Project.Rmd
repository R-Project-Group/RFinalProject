---
title: "Final Project"
author: "Phillip Ross, Ming Chen, Hersh Agarwal"
date: "5/6/2021"
output: html_document
---

# What are the drivers of police killings in the US?

## Executive Summary

## Goal of the study
Police killings have been on the forefront of the news in the recent years and is a prominent issue the United States is facing today. Our objective in this study is to identify insights within this dataset that may illuminate drivers of killings and identify possible biases in how police use lethality in their techniques.

## Data sources
The primary data-set our study is based on is a police killings database maintained by the Washington Post. The data is comprehensive starting from January 1st, 2015 through early May 2021, compiled from local news reports, law enforcement websites, and mining independent databases dedicated to tracking police killings. See the source article here: https://www.washingtonpost.com/graphics/investigations/police-shootings-database/ and the available data here: https://github.com/washingtonpost/data-police-shootings. 

To supplement this data, we utilized the Census Bureau data tool to download demographic and economic information available at the county level and joined it with the killings data by county (or FIPS code). The Census Bureau has a data query tool that appears fairly comprehensive but is also a bit un-intuitive to use. Nonetheless, we pulled in five different county-level data-sets from the American Community Survey which was last taken in 2019. These reports include 1) Demographics and Housing, Educational Attainment, Financial Characteristics, Selected Economics Characteristics, Selected Housing Characteristics. 

We then assign killings to each county using longitude and latitude data, and then joined the census data, such as income, race, and population data for each county with the killings data by FIPS County Code. Our focus was largely at the county level, since it was difficult to get the data necessary from the census at the city level, and the state level was not granular enough. We performed the following  analyses to understand insights from the data.

## List of Analyses

1. We examined whether certain counties had a disproportionate percentage of the total killings using a Pareto chart. 
2. We graphed killings by year to see whether there was a time series trend. 
3. We looked at the counties with the highest killing rate to see if there was any trend among them. 
4. We mapped all counties, over the full six completed year period (2015-2020) and by year to see if there was any discernable trend. 
5. We looked at killings by race. 
6. We used model selection (LASSO) to come up a multi-variate model to try and predict the factors to lead to higher rates of police killings.  

## Findings

The summary of our findings is below. 

1. Certain counties, especially larger counties such as LA County, make up a disproportionate number of the absolute number of police killings, which is unsurprising. 
2. The absolute number of police killings has bounced around over the last 6 years, with 2015 being the highest. If anything, odd years have a higher number of killings, which is likely just random noise.  
3. While difficult to note a trend among the 10 counties with the highest rates of police killings, we did note that all 10 were in the West (or at least Midwest, depending on how you classify Missouri and Oklahoma).  
4. There was again no discernable trend by year, however again most of the high killing rate counties were in the West and Southwest. 
5. Unsurprisingly, given the backdrop of this project, African Americans were killed at the highest rates relative to their population, at 2x the rate of the next highest race, Latinos. Whites were killed at a slightly lower rate than Latinos, while Asians faced by far the lowest rate of police killings.  
6. Our model came up with x factors at the 0.05 significance level cut off, with an overall R squared of x, which for real world data is a decent result. X, X, X were factors that increased the rate of police killings while x, x and x were factors that decreased the rate of police killings.   

## Caveats & Next Steps

There were several drawbacks to our data. Not all of the police killings in the Washington Post data had longitudes and latitudes, so we had to remove them (~300 killings out of 6,300).

One of the next analyses we would like to do would be to repeat our analyses for total killings for counties with disproportionate African American killings, to see whether different factors lead to higher rates of African American killings compared to overall level of killings. We would also like to do further research to see if we could isolate other sources of data to analyze killings at the city level, which may remove some of the noise at the county level (for example, if certain counties had just one city vs. multiple cities).  

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require('pacman')) {install.packages('pacman')}
pacman::p_load(tidyverse, stringr, dplyr, ggplot2, ggthemes, data.table, lubridate,
               RColorBrewer, ggsci, plotROC, usmap,
               plotly, ggpubr, vistime, RCurl, RJSONIO, sf, tigris, devtools, gtsummary, cli, skimr)
library(dplyr)
```

## R Markdown

## Exploratory Data Analysis and Data Preparation

### Read in Data

These are census shape files which we need to map latitude and longitutde to FIPs codes
```{r}
census_tracts <- st_read("CensusShapes2/cb_2018_us_county_within_cd116_500k.shp", quiet = TRUE)
```

This is the police killing data from Kaggle
```{r}
killdata <- read.csv("PoliceKillingsUS_MC.csv")
```

This is a look-up table that maps FIPS code to County name and State (from: https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697)
```{r}
fipsLookUp <- read.csv("FIPSCountyStateLookUp.csv", colClasses ='character') 
fipsLookUp$fips <- fipsLookUp[,1] # Import has made the FIPS column name weird so re-making column
```

First-off, we need take our geo-coordinates of each police killing and map it to a county (uniquely identified by a FIPS code). This is because all the census data we plan to join in is readily available at the county level

There are APIs that do this, but we couldn't figure out how to get them to work. Instead, we found a tutorial that relies on plotting lat/long within county boundaries as defined in the shape files. Tutorial here: https://shiandy.com/post/2020/11/02/mapping-lat-long-to-fips/

```{r message=FALSE, warning=FALSE}
# Remove the lat/long from the killing data to feed into this algorithm
killdata_latlong <- data.frame(latitude = killdata$latitude, longitude = killdata$longitude)

# Remove observations without lat/long and convert them into a shape file
latlong_sf <- killdata_latlong %>%
  filter(!is.na(latitude), !is.na(longitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = st_crs(census_tracts))

# Check the overlap between the lat/long and the census tracts
intersected <- st_intersects(latlong_sf, census_tracts) 

# Indicate overlap and if there is overlap, bring in the GEOID
latlong_final <- latlong_sf %>%
  mutate(intersection = as.integer(intersected),
         fips = if_else(is.na(intersection), "",
                        census_tracts$GEOID[intersection]))

# GEOID does not seem to be FIPS exactly -- pull the first two and last three characters to get to the FIPS
latlong_final$mod_fips <- paste(str_sub(latlong_final$fips, 1, 2), str_sub(latlong_final$fips, -3, -1), sep = "") 

# We remove all killings that don't have a latitude/longitude. We go from 6268 observations to 5963 observations which is not a terrible fall-out.
killdata_clean <- killdata %>%
  filter(!is.na(latitude), !is.na(longitude)) # Remove all killings that don't have latitude/longitude

# We take the FIPs code that we have generated from the lat/long and append it onto the data
killdata_clean$mod_fips <- latlong_final$mod_fips  
```

The next step is to read-in the census data. All data was sourced from the American Community Survey in 2019 and pulled at the FIPS level for all counties
```{r}
# Skip the first row, as it is a redundant header row
demoData <- read.csv("Data (County-Level)/Demographics and Housing/ACSDP1Y2019.DP05_data_with_overlays_2021-04-29T104649.csv", skip = 1, header = T)
# ACS DEMOGRAPHIC AND HOUSING ESTIMATES

eduData <- read.csv("Data (County-Level)/Educational Attainment/ACSST1Y2019.S1501_data_with_overlays_2021-05-07T192056.csv", skip = 1, header = T)
# ACS EDUCATIONAL ATTAINMENT

financeData <- read.csv("Data (County-Level)/Financial Characteristics/ACSST1Y2019.S2503_data_with_overlays_2021-05-07T193752.csv", skip = 1, header = T)
# ACS FINANCIAL CHARACTERISTICS

econData <- read.csv("Data (County-Level)/Selected Economic Characteristics/ACSDP1Y2019.DP03_data_with_overlays_2021-05-07T193216.csv", skip = 1, header = T)
# ACS SELECTED ECONOMIC CHARACTERISTICS

housingData <- read.csv("Data (County-Level)/Selected Housing Characteristics/ACSDP1Y2019.DP04_data_with_overlays_2021-04-29T141348.csv", skip = 1, header = T)
# ACS SELECTED HOUSING CHARACTERISTICS
```

Let's now join together all the census data by ID (in this case the FIPS)
```{r}
masterCensus <- demoData %>%
  left_join(econData, by="id") %>%
  left_join(eduData, by="id") %>%
  left_join(housingData, by="id") %>%
  left_join(financeData, by="id")
```

Half of the columns are columns which estimate how much error they may have been in the measurement. These are a bit too complex to work with, so let's just remove them

There are also overlapping columns across each set. These get a '.y'. or '.x' added to them so can be identified and removed.  

```{r}
masterCensus_s2 <- masterCensus %>%
  select(-contains("Error")) %>% # Removes columns that indicator the error
  select(-contains(".y")) %>% # Indicates a duplicated variable
  select(-contains(".x"))
```

### Data exploration

Let's look at the killings data to understand what data we have, and what sort of dependent variable we want to create. 
```{r message=FALSE, warning=FALSE}
killdata_clean %>%
  group_by(manner_of_death, signs_of_mental_illness, threat_level, flee, body_camera) %>%
  summarise(count = n())

killdata_clean %>%
  group_by(manner_of_death) %>%
  summarise(count = n()) # Two options: 'shot' and 'shot and Tasered' (low freq.) can roll-up

killdata_clean %>%
  group_by(signs_of_mental_illness) %>%
  summarise(count = n()) # Roughly 25% of victims had signs of mental illness (Y or N)

killdata_clean %>%
  group_by(threat_level) %>%
  summarise(count = n()) # Three categories, 'attack', 'other' and 'undetermined' (low freq)

killdata_clean %>%
  group_by(flee) %>%
  summarise(count = n()) # Five options: a blank, 'Car', 'Foot', 'Not fleeing' and 'Other'; potentially roll-up to fleeing and not fleeing

killdata_clean %>%
  group_by(body_camera) %>%
  summarise(count = n()) # Options FALSE or TRUE. Only ~14% of deaths had body cameras
```

Given that there is not much data, it may not make sense to differentiate different types of killings (e.g. those where there was a threat or signs of mental illness). It may not make much sense to differentiate by year either. 

We were curious to see how killing is disseminated across countries represented in the data. Below is a code to construct a Pareto chart of killing by FIPS. This shows us whether few counties represent most of the killing. Pareto tutorial sourced from here: https://rstudio-pubs-static.s3.amazonaws.com/72023_670962b57f444c04999fd1a0a393e113.html

```{r}
killdata_FIPS <- killdata_clean %>% # Create a county-level data-set with # of killings
  group_by(mod_fips) %>%
  summarise(totalKillings = n()) %>%
  rename(fips = mod_fips)

paretoData <- arrange(killdata_FIPS, desc(totalKillings)) %>%
  mutate(cumsum = cumsum(totalKillings),
         freq = round(totalKillings/sum(totalKillings), 3), 
         cum_freq = cumsum(freq))

## Saving Parameters 
def_par <- par()

# New margins
par(mar=c(5,5,4,5))

## plot bars, pc will hold x values for bars
pc = barplot(paretoData$totalKillings,  
             width = 1, space = 0.2, border = NA, axes = F,
             ylim = c(0, 1.05 * max(paretoData$totalKillings, na.rm = T)), 
             ylab = "Cumulative Counts" , cex.names = 0.7, 
             names.arg = paretoData$fips,
             main = "Killings by County")

## annotate left axis
axis(side = 2, at = c(0, paretoData$totalKillings), las = 1, col.axis = "grey62", col = "grey62", tick = T, cex.axis = 0.8)

## frame plot
box( col = "grey62")

## Cumulative Frequency Lines 
lines(pc, paretoData$cumsum, type = "b", cex = 0.7, pch = 19, col="cyan4")

## Annotate Right Axis
px <- paretoData$cum_freq * max(paretoData$totalKillings, na.rm = T)
lines(pc, px, type = "b", cex = 0.7, pch = 19, col="cyan4")

## restoring default parameter
axis(side = 4, at = c(0, px), labels = paste(c(0, round(paretoData$cum_freq * 100)) ,"%",sep=""), 
     las = 1, col.axis = "grey62", col = "cyan4", cex.axis = 0.8, col.axis = "cyan4")
```

One FIPs has a staggeringly high killing number (255), but we have to scale this by population. There is a very long right tail and from the pareto shape it does seem like certain counties are over-represented.

Which county is this?
```{r}
killdata_FIPS$fips[which.max(killdata_FIPS$totalKillings)]
killdata_FIPS$totalKillings[which.max(killdata_FIPS$totalKillings)]
```
The maximum number of total killings is in Los Angeles County at 255. 

Have killings increased over time? We look at this broadly here.
```{r}
# Evaluate data across time
killdata_fips_year <- killdata_clean %>%
  mutate(date = as.Date(killdata_clean$date, format = "%m/%d/%Y")) %>%
  mutate(year = lubridate::year(date)) %>%
  group_by(year) %>%
  summarise(totalKillings = n())
```
There is not much difference year over year; might not make sense to separate this by years

### Continued Data Processing

```{r}
# Pull out the FIPS code out of the ID identified in the census data
masterCensus_s2$fips <- str_sub(masterCensus_s2$id, -5, -1)
```

We will join the killings data to the demographics and population data from the census. The response variable we are creating will be called the killing rate and is the number of killings (across all time) divided by the 2019 county population. Because this rate is very low, we've also created a transformed version (log()) which we will use for the subsequent data analysis. 

To not exclude countries that have 0 killings (which would be undefined with log(0)), we have coalesced any county that has a 0% killing rate into having a 0.0001% killing rate.

```{r}
sdl1 <- masterCensus_s2 %>%
    left_join(killdata_FIPS, by="fips") %>%
    mutate(killingRate =  totalKillings/Estimate..SEX.AND.AGE..Total.population) %>%
    mutate(killingRateEff = coalesce(killingRate, 0.000001)) %>%
    mutate(killingRateLog = log(killingRateEff)) %>%
    select(fips,totalKillings,killingRate,killingRateLog,everything())
```

### Data cleaning

Although the census data is somewhat standardized it isn't useable in its raw form. 

Step 1 cleaning; only keep columns that have 'percentage' data. Countries differ greatly by size, so the nominal data could push results one way or another. By using percentage data, each variable in each county receives equal weight
```{r}
sdl2 <- sdl1 %>%
    select(contains("Percent"))
```

Step 2: identify non-numeric values within the census data and remove such variables
```{r}
table(unlist(sdl2, use.names = FALSE)[(!grepl('^[0-9]',unlist(sdl2, use.names = FALSE)))])
```

It looks like things are coded (X) or N if they are missing; let's look at which columns have a high percentage of (X) or N and are thus worth dropping
```{r message=FALSE, warning=FALSE}
# Flag any column where more than half the values are '(X)'
library(plyr) # plyr can cause issues -- we advise clearing directory and only loading plyr at this point
count.X.per.column <- ldply(sdl2, function(c) sum(c=="(X)"))
columnsToExclude1 <- count.X.per.column$.id[count.X.per.column$V1 > 420]

# Flag any column where more than half of the values are 'N'
count.N.per.column <- ldply(sdl2, function(c) sum(c=="N"))
columnsToExclude2 <- count.N.per.column$.id[count.N.per.column$V1 > 420]

# Drop columns with too many NULL values
sdl3 <- sdl2 %>%
  select(-columnsToExclude1)

sdl4 <- sdl3 %>%
  select(-columnsToExclude2)
```

Step 3: Convert remaining data into numeric values (this will force some values to NA). In this case, we're comfortable with that.
```{r message=FALSE, warning=FALSE}
# Convert remaining data frame into numeric columns (will force some values to NA)
sdl5 <- mutate_all(sdl4, function(x) as.numeric(as.character(x)))
```

Step 4: Eliminate data columns where the maximum or minimum values exceeds the allowable amount. For example, some columns are labeled as 'percent' but actually contain nominal values.
```{r message=FALSE, warning=FALSE}
# Flag any column where the minimum is above 100 (percentage cannot be over 100)
colMin <- apply(sdl5, 2, min, na.rm = TRUE)
columnsToExclude3 <- names(colMin)[colMin>100]

# Drop columns with values that are below the expected range 
sdl6 <- sdl5 %>%
  select(-columnsToExclude3)

# Flag any column where the maximum is above 100 (percentage cannot be over 100)
colMax <- apply(sdl6, 2, max, na.rm = TRUE)
columnsToExclude4 <- names(colMax)[colMax>100]

# Drop columns with value that exceeds the expected range
sdl7 <- sdl6 %>%
  select(-columnsToExclude4)
```

Step 5: Our cleaning is aggressive. And there may be some variables that we think are worth keeping despite them not meeting our original tests. For example, we want to control for total population (a nominal variable) so we will keep it, despite it not being a percentage. After reviewing the first model, we realized there is a key issue with many variables being correlated. We created a manual short-list of 70-80 variables that we sub-select for the model, if they pass the cleaning steps above. 
```{r}
manualForceIn <- read.csv('census_columns_keep.csv.', header = FALSE)
manualForceIn[1,] <- "Estimate..SEX.AND.AGE..Total.population"

manualForceInCols <- intersect(names(sdl7), manualForceIn[,1])

sdl7 <- sdl7[, manualForceInCols]

# Re-add response variables back to cleaned census data
sdl7$fips <- sdl1$fips
sdl7$totalKillings <- sdl1$totalKillings
sdl7$killingRate <- sdl1$killingRate
sdl7$killingRateLog <- sdl1$killingRateLog

# Re-add key predictor variables which were excluded
sdl7$fips <- sdl1$fips
sdl7$Estimate..GROSS.RENT..Occupied.units.paying.rent..Median..dollars. <- sdl1$Estimate..GROSS.RENT..Occupied.units.paying.rent..Median..dollars.
# sdl7$Estimate..HOUSING.OCCUPANCY..Total.housing.units..Homeowner.vacancy.rate <- sdl1$Estimate..HOUSING.OCCUPANCY..Total.housing.units..Homeowner.vacancy.rate
# sdl7$Estimate..HOUSING.OCCUPANCY..Total.housing.units..Rental.vacancy.rate <- sdl1$Estimate..HOUSING.OCCUPANCY..Total.housing.units..Rental.vacancy.rate
sdl7$Estimate..HOUSING.TENURE..Occupied.housing.units..Average.household.size.of.owner.occupied.unit <- sdl1$Estimate..HOUSING.TENURE..Occupied.housing.units..Average.household.size.of.owner.occupied.unit
sdl7$Estimate..HOUSING.TENURE..Occupied.housing.units..Average.household.size.of.renter.occupied.unit <- sdl1$Estimate..HOUSING.TENURE..Occupied.housing.units..Average.household.size.of.renter.occupied.unit
sdl7$Estimate..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Per.capita.income..dollars. <- sdl1$Estimate..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Per.capita.income..dollars.
sdl7$Estimate..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households..Median.household.income..dollars. <- sdl1$Estimate..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households..Median.household.income..dollars.
sdl7$Estimate..ROOMS..Total.housing.units..Median.rooms <- sdl1$Estimate..ROOMS..Total.housing.units..Median.rooms
sdl7$Estimate..SELECTED.MONTHLY.OWNER.COSTS..SMOC...Housing.units.with.a.mortgage..Median..dollars. <- sdl1$Estimate..SELECTED.MONTHLY.OWNER.COSTS..SMOC...Housing.units.with.a.mortgage..Median..dollars.
sdl7$Estimate..SELECTED.MONTHLY.OWNER.COSTS..SMOC...Housing.units.without.a.mortgage..Median..dollars. <- sdl1$Estimate..SELECTED.MONTHLY.OWNER.COSTS..SMOC...Housing.units.without.a.mortgage..Median..dollars.
sdl7$Estimate..SEX.AND.AGE..Total.population <- sdl1$Estimate..SEX.AND.AGE..Total.population
sdl7$Estimate..VALUE..Owner.occupied.units..Median..dollars. <- sdl1$Estimate..VALUE..Owner.occupied.units..Median..dollars.

detach("package:plyr", unload = TRUE) # we try to detatch plyr so it doesn't mess up our dplyr functions later

# Join on county and state variables onto data layer -- did this in case it would be helpful for later analyses
sdl8 <- sdl7 %>%
  left_join(fipsLookUp, by="fips") %>%
  select(fips,County,State,totalKillings,killingRate,killingRateLog,everything()) %>%
  select(-Ã¯..FIPS)
```

### Model Selection

Just like in class, we will use LASSO and the glmnet function to select a more parsimonious set of variables of the 70 or so that we have. We played around with a with parameters here (including a model that had 369 variables and then our sub-set of 70 variables)
```{r}
# Remove non-predictive variables
sdl8sub = sdl8[,-1:-5]

# Glmnet doesn't play with NAs, so coalesce all NAs into 0
sdl9sub <- sdl8sub %>%
   mutate_all(~replace(., is.na(.), 0)) 

set.seed(1)

# Create model matrix for GLM
X.fl <- model.matrix(killingRateLog~., data=sdl9sub)[,-1]
Y <- sdl9sub$killingRateLog

# Run CV glmnet with ten-folds
fit.lasso.0 <- glmnet::cv.glmnet(X.fl,Y, nfolds = 10, intercept =T)
plot(fit.lasso.0)
```

The plot suggests that MSE is minimized around 56 variables, but we will choose the 1SE point to reduce the amount of variables brought into the model.
```{r}
# Identify variables associated with MSE at 1SE above minimum
coef.1se <- coef(fit.lasso.0, s="lambda.1se")
var.1se <- coef.1se@Dimnames[[1]][coef.1se@i + 1][-1]

# Extract selected variables out of data-set
data.fl.sub <- sdl9sub[,c("killingRateLog", var.1se)] # get a 

# Re-name columns so that the regression output is more presentable
data.fl.sub2 <- data.fl.sub %>%
  rename(Pop_OneRace = Percent..RACE..Total.population..One.race) %>%
  rename(Pop_White = Percent..Race.alone.or.in.combination.with.one.or.more.other.races..Total.population..White) %>%
  rename(Pop_AmericanIndianAlaskan = Percent..Race.alone.or.in.combination.with.one.or.more.other.races..Total.population..American.Indian.and.Alaska.Native) %>%
  rename(Pop_HispanicLatino = Percent..HISPANIC.OR.LATINO.AND.RACE..Total.population..Hispanic.or.Latino..of.any.race.) %>%
  rename(Income_25to35k = Percent..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households...25.000.to..34.999) %>%
  rename(Income_50to75k = Percent..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households...50.000.to..74.999) %>%
  rename(Income_wSocialSec = Percent..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households..With.Social.Security) %>%
  rename(Income_wSuppSec = Percent..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households..With.Supplemental.Security.Income) %>%
  rename(Income_foodStamp = Percent..INCOME.AND.BENEFITS..IN.2019.INFLATION.ADJUSTED.DOLLARS...Total.households..With.Food.Stamp.SNAP.benefits.in.the.past.12.months) %>% 
  rename(HealthCoverage = Percent..HEALTH.INSURANCE.COVERAGE..Civilian.noninstitutionalized.population..With.health.insurance.coverage) %>%
  rename(PrivHealthCov = Percent..HEALTH.INSURANCE.COVERAGE..Civilian.noninstitutionalized.population..With.health.insurance.coverage..With.private.health.insurance) %>%
  rename(noHealthCov = Percent..HEALTH.INSURANCE.COVERAGE..Civilian.noninstitutionalized.population..No.health.insurance.coverage) %>%
  rename(edu_BlackHighSchool = Estimate..Percent..RACE.AND.HISPANIC.OR.LATINO.ORIGIN.BY.EDUCATIONAL.ATTAINMENT..Black.alone..High.school.graduate.or.higher) %>%
  rename(vacantHousing = Percent..HOUSING.OCCUPANCY..Total.housing.units..Vacant.housing.units) %>%
  rename(duplexes = Percent..UNITS.IN.STRUCTURE..Total.housing.units..2.units) %>%
  rename(mdus = Percent..UNITS.IN.STRUCTURE..Total.housing.units..5.to.9.units) %>%
  rename(mobileHomes = Percent..UNITS.IN.STRUCTURE..Total.housing.units..Mobile.home) %>%
  rename(twoVehiclesAvailable = Percent..VEHICLES.AVAILABLE..Occupied.housing.units..2.vehicles.available) %>%
  rename(noPlumbing = Percent..SELECTED.CHARACTERISTICS..Occupied.housing.units..Lacking.complete.plumbing.facilities) %>%
  rename(houseswMortages = Percent..MORTGAGE.STATUS..Owner.occupied.units..Housing.units.with.a.mortgage) %>%
  rename(medianRent = Estimate..GROSS.RENT..Occupied.units.paying.rent..Median..dollars.) %>%
  rename(roomsInHousing = Estimate..ROOMS..Total.housing.units..Median.rooms) %>%
  rename(ownerCostsHousing = Estimate..SELECTED.MONTHLY.OWNER.COSTS..SMOC...Housing.units.without.a.mortgage..Median..dollars.) %>%
  rename(totalPop = Estimate..SEX.AND.AGE..Total.population)
```

We now run the relaxed LASSO. 
```{r}
# Use selected variables in Relaxed-LASSO
fit.min.lm <- lm(killingRateLog~., data=data.fl.sub2)

# We compare this against the model with the full set of variables
fit.all.lm <- lm(killingRateLog~., data=sdl9sub)
# summary(fit.all.lm)
# R^2 is only 0.3261

summary(fit.min.lm)
```
The model has a R-squared of 0.2709 which is not high, but potentially has some predictive ability. Several variables selected from LASSO are significant.

We have run some diagnostic plots also to assess the regression. Conditions for heteroskedasticity and normality of residuals seem appeased

```{r}
# Diagnostic plots
plot(fit.min.lm$fitted, fit.min.lm$residuals,
pch = 16,
main = "residual plot")
abline(h=0, lwd=4, col="red")

qqnorm(fit.min.lm$residuals)
```

```{r}
# Runs a linear chart for killings each year
p <- ggplot(data=killdata_fips_year, aes(x=year, y=totalKillings)) +
    geom_line() +
    geom_point()
p + xlim(c(2015, 2020)) +ylim(c(900, 1000))
```

There does not seem to be any discernable pattern in killings from year to year (other than 2015 having a higher level of killings). 

```{r}
# installs urbnmapr package
devtools::install_github("UrbanInstitute/urbnmapr")
```

```{r}
#creates a dataframe of all counties from urbanmapr
counties_sf <- urbnmapr::get_urbn_map(map = "counties", sf = TRUE)
head(counties_sf)
```

```{r}
#creates new data frame that groups killings by fips only
killdata_fips_county <-
  killdata_clean %>%
  mutate(date = as.Date(killdata_clean$date, format = "%m/%d/%Y")) %>%
  mutate(year = lubridate::year(date)) %>%
  group_by(mod_fips) %>%
  summarise(totalKillings = n()) %>%
  rename(fips = mod_fips)
```

```{r}
#joins that data frame with census data to get a killing rate
fulldata <- left_join(killdata_fips_county, masterCensus_s2, by="fips") %>%
  mutate(killingRate =  (totalKillings/Estimate..SEX.AND.AGE..Total.population)*100000/6) %>%
  mutate(killingRateEff = coalesce(killingRate, 0.000001)) %>%
  mutate(killingRateLog = log(killingRateEff)) %>%
  arrange(desc(killingRate)) %>%
  rename(county_fips = fips) %>%
  select(county_fips,totalKillings,killingRate,killingRateLog)
head(fulldata,10)
```
The 10 highest county annual killing rates over the six year period of 2015-2020 are: Tehama, CA, Pueblo, CO, Fairbanks North Star, AK, Shasta, CA, Muskogee, OK, St. Louis, MO, Lea, NM, Coconino, AZ, Apache, AZ, Yellowstone, MT. It's hard to discern commonalities among these ten counties, but one clear trend is that they are almost all in the Western part of the United States. None of them are in major cities, either. 

```{r}
#joins the data frame with FIPs and killing rate with the county map
county_map <- left_join(counties_sf, fulldata, by = "county_fips", copy = TRUE)

#filters to make all FIPS with zero killing 0
county_map_filtered <- 
  county_map %>%
  mutate_all(~replace(., is.na(.), 0))

#plots the map
county_map_filtered %>%
  ggplot() +
  geom_sf(mapping = aes(fill = killingRate),
          color = NA, size = 0.05) +
  labs(fill = "Annual Rate of Police Killings") + 
  scale_fill_distiller(palette = "Spectral")
```
It looks like generally, the more populous areas of the country have higher rates of killing (lots of Midwestern counties with zero killings). The southwest in particular seems to be an area with especially higher rates of killing. 

```{r}
#creates new data frame that groups killings by fips AND year
killdata_fips_yearcounty <-
  killdata_clean %>%
  mutate(date = as.Date(killdata_clean$date, format = "%m/%d/%Y")) %>%
  mutate(year = lubridate::year(date)) %>%
  group_by(mod_fips, year) %>%
  summarise(totalKillings = n()) %>%
  rename(fips = mod_fips)
```

```{r}
#joins that data frame with census data to get a killing rate
fulldata2 <- left_join(killdata_fips_yearcounty, masterCensus_s2, by="fips") %>%
  mutate(killingRate =  (totalKillings/Estimate..SEX.AND.AGE..Total.population)*100000) %>%
  mutate(killingRateEff = coalesce(killingRate, 0.000001)) %>%
  mutate(killingRateLog = log(killingRateEff)) %>%
  rename(county_fips = fips) %>%
  select(county_fips,totalKillings,killingRate, killingRateLog, year)
```


```{r}
county_map2 <- left_join(counties_sf, fulldata2, by = "county_fips", copy = TRUE)
county_map_filtered2 <- 
  county_map2 %>%
  filter(year < 2021 & year >2014) %>%
  mutate_all(~replace(., is.na(.), 0)) 


county_map_filtered2 %>%
  ggplot() +
  geom_sf(mapping = aes(fill = killingRate),
          color = NA, size = 0.05) +
  labs(fill = "Annual Rate of Police Killings") + 
  scale_fill_distiller(palette = "Spectral") +
  facet_wrap(~year)

```

It is again difficult to discern a particular pattern between years, but it does appear that in the majority of the years, the West has many of the counties with the highest rate of Police Killings. 

Analysis of black killings per capita vs. overall killings per capita
```{r}

killdata_raceanalysis <- killdata_clean %>%
  rename(fips = mod_fips) %>%
  filter(race == 'B') %>%
  group_by(fips) %>%
  summarise(totalBlackKillings = n()) %>%
  left_join(masterCensus_s2, by="fips") %>%
  mutate(blackPop = as.numeric(Percent..Race.alone.or.in.combination.with.one.or.more.other.races..Total.population..Black.or.African.American)/100*as.numeric(Estimate..SEX.AND.AGE..Total.population)) %>%
  na.omit() %>%
  mutate(blacksRatio = as.numeric(totalBlackKillings)/as.numeric(blackPop)*100000) %>%
  select(fips, totalBlackKillings, blacksRatio, blackPop)


killdata_FIPS2 <- killdata_FIPS %>%
  left_join(masterCensus_s2, by="fips") %>%
  na.omit() %>%
  mutate(overallRatio = as.numeric(totalKillings)/as.numeric(Estimate..SEX.AND.AGE..Total.population)*100000) %>%
  select(fips, totalKillings, overallRatio, Estimate..SEX.AND.AGE..Total.population,) %>%
  rename(overallPop = Estimate..SEX.AND.AGE..Total.population)


killdata_raceanalysis <- killdata_raceanalysis %>%
  left_join(killdata_FIPS2, by="fips") %>%
  mutate(ratioDiff = blacksRatio - overallRatio)
  

uswhitedeaths <- sum(killdata_clean$race == 'W', na.rm=TRUE)
uswhitepop <- masterCensus_s2$Estimate..Race.alone.or.in.combination.with.one.or.more.other.races..Total.population..White %>%
  as.numeric()
uswhitepop <- sum(uswhitepop, na.rm=TRUE)
usblackdeaths <- sum(killdata_clean$race == 'B', na.rm=TRUE)
usblackpop <- masterCensus_s2$Estimate..Race.alone.or.in.combination.with.one.or.more.other.races..Total.population..Black.or.African.American %>%
  as.numeric()
usblackpop <- sum(usblackpop, na.rm=TRUE)
ushispanicdeaths <- sum(killdata_clean$race == 'H', na.rm=TRUE)
ushispanicpop <- masterCensus_s2$Estimate..HISPANIC.OR.LATINO.AND.RACE..Total.population..Hispanic.or.Latino..of.any.race. %>%
  as.numeric()
ushispanicpop<- sum(ushispanicpop, na.rm=TRUE)
usasiandeaths <- sum(killdata_clean$race == 'A', na.rm=TRUE)
usasianpop <- masterCensus_s2$Estimate..Race.alone.or.in.combination.with.one.or.more.other.races..Total.population..Asian %>%
  as.numeric()
usasianpop <- sum(usasianpop, na.rm=TRUE)


uswhitedeaths/uswhitepop*100000
usblackdeaths/usblackpop*100000
ushispanicdeaths/ushispanicpop*100000
usasiandeaths/usasianpop*100000


```

On a national level, there are clear racial disparities among the per capita rate of police killings. When looking at the number of killings of a particular race per 10K population of that race, we can see that black Americans are killed at over 2x the rate of white Americans. Those of Hispanic descent are killed about 1/3 higher rate than white Americans. Asian Americans are killed at a significantly lower rate by police than any other racial group. While there are certainly many other factors that contribute to a difference in the rate of police killings, the huge disparity suggests that race should be considered as a factor when discussing how to tackle this issue.


```{r}

plot(x = killdata_raceanalysis$blacksRatio, 
     y = killdata_raceanalysis$overallRatio, 
     pch  = 16,     # "point character": shape/character of points 
     cex  = 0.8,    # size
     col  = "blue", # color 
     xlab = "Black Killings per 100K Black Population",  # x-axis
     ylab = "Overall Killings per 100K General Population",  # y-axis 
     main = "Comparison of Killings/100K Population by County",
     xlim=c(0,30),
     ylim=c(0,11),
     abline(coef = c(0,1)))

```


This scatter plot displays counties in the US along the ratio of black Americans killed by the ratio of the general population killed. The line dividing the plot has a slope of one, so counties alongside the line have a black ratio equal to the general ratio of killing per 10K residents. There are a few takeaways from this plot. First, it's clear that this is a widespread issue plaguing much of the United States, and is not a result of a few "bad areas" driving up the racial disparity, demonstrated by the vast number of points to the right of the line. Second, there are in fact many counties where there are extremely high levels of racial disparity where the federal or state government may need to review police training at the local levels.