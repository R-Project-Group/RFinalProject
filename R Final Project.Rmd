---
title: "Final Project"
author: "Phillip Ross, Ming Chen, Hersh Agarwal"
date: "5/6/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require('pacman')) {install.packages('pacman')}
pacman::p_load(tidyverse, dplyr, ggplot2, ggthemes, data.table, lubridate,
               GGally, RColorBrewer, ggsci, plotROC, usmap,
               plotly, ggpubr, vistime, RCurl, RJSONIO, sf, tigris)
```

## R Markdown

```{r}
# Read-in data

# These are census shape files which we need to map latitude and longitutde to FIPs codes
census_tracts <- st_read("CensusShapes2/cb_2018_us_county_within_cd116_500k.shp", quiet = TRUE)

# This is the police killing data from Kaggle
killdata <- read.csv("PoliceKillingsUS_MC.csv")

# This is a look-up table that maps FIPS code to County name and State (from: https://www.nrcs.usda.gov/wps/portal/nrcs/detail/national/home/?cid=nrcs143_013697)
fipsLookUp <- read.csv("FIPSCountyStateLookUp.csv", colClasses ='character') 
fipsLookUp$fips <- fipsLookUp[,1] # Import has made the FIPS column name weird so re-making column

# First-off, we need take our geo-coordinates of each police killing and map it to a county (uniquely identified by a FIPS code). This is because all the census data we plan to join in is readily available at the county level

# There are APIs that do this, but we couldn't figure out how to get them to work. Instead, we found a tutorial that relies on plotting lat/long within county boundaries as defined in the shape files. Tutorial here: https://shiandy.com/post/2020/11/02/mapping-lat-long-to-fips/

# Remove the lat/long from the killing data to feed into this algorithm
killdata_latlong <- data.frame(latitude = killdata$latitude, longitude = killdata$longitude)

# Remove observations without lat/long and convert them into a shape file
latlong_sf <- killdata_latlong %>%
  filter(!is.na(latitude), !is.na(longitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = st_crs(census_tracts))

# Check the overlap between the lat/long and the census tracts
intersected <- st_intersects(latlong_sf, census_tracts) 

# Indicate overlap and if there is overlap, bring in the GEOID
latlong_final <- latlong_sf %>%
  mutate(intersection = as.integer(intersected),
         fips = if_else(is.na(intersection), "",
                        census_tracts$GEOID[intersection]))

# GEOID does not seem to be FIPS exactly -- pull the first two and last three characters to get to the FIPS
latlong_final$mod_fips <- paste(str_sub(latlong_final$fips, 1, 2), str_sub(latlong_final$fips, -3, -1), sep = "") 

# We remove all killings that don't have a latitude/longitude. We go from 6268 observations to 5963 observations which is not a terrible fall-out.
killdata_clean <- killdata %>%
  filter(!is.na(latitude), !is.na(longitude)) # Remove all killings that don't have latitude/longitude

# We take the FIPs code that we have generated from the lat/long and append it onto the data
killdata_clean$mod_fips <- latlong_final$mod_fips  
```

```{r}
# Read-in census data. All data was sourced from the American Community Survey in 2019 and pulled at the FIPS level for all counties

# Skip the first row, as it is a redundant header row
demoData <- read.csv("Data (County-Level)/Demographics and Housing/ACSDP1Y2019.DP05_data_with_overlays_2021-04-29T104649.csv", skip = 1, header = T)
# ACS DEMOGRAPHIC AND HOUSING ESTIMATES

eduData <- read.csv("Data (County-Level)/Educational Attainment/ACSST1Y2019.S1501_data_with_overlays_2021-05-07T192056.csv", skip = 1, header = T)
# ACS EDUCATIONAL ATTAINMENT

financeData <- read.csv("Data (County-Level)/Financial Characteristics/ACSST1Y2019.S2503_data_with_overlays_2021-05-07T193752.csv", skip = 1, header = T)
# ACS FINANCIAL CHARACTERISTICS

econData <- read.csv("Data (County-Level)/Selected Economic Characteristics/ACSDP1Y2019.DP03_data_with_overlays_2021-05-07T193216.csv", skip = 1, header = T)
# ACS SELECTED ECONOMIC CHARACTERISTICS

housingData <- read.csv("Data (County-Level)/Selected Housing Characteristics/ACSDP1Y2019.DP04_data_with_overlays_2021-04-29T141348.csv", skip = 1, header = T)
# ACS SELECTED HOUSING CHARACTERISTICS
```

```{r}
# names(demoData)

# Join together all the demo data by ID (in this case the FIPS)
masterCensus <- demoData %>%
  left_join(econData, by="id") %>%
  left_join(eduData, by="id") %>%
  left_join(housingData, by="id") %>%
  left_join(financeData, by="id")

# Half of the columns are columns which estimate how much error they may have been in the measurement. These are a bit too complex to work with, so let's just remove them
# There are also overlapping columns across each set. These get a '.y'. or '.x' added to them so can be identified and removed.  
masterCensus_s2 <- masterCensus %>%
  select(-contains("Error")) %>% # Removes columns that indicator the error
  select(-contains(".y")) %>% # Indicates a duplicated variable
  select(-contains(".x"))
```

```{r}
# Let's look at the killings data to understand what data we have, and what sort of dependent variable we want to create. 

killdata_clean %>%
  group_by(manner_of_death, signs_of_mental_illness, threat_level, flee, body_camera) %>%
  summarise(count = n())

killdata_clean %>%
  group_by(manner_of_death) %>%
  summarise(count = n()) # Two options: 'shot' and 'shot and Tasered' (low freq.) can roll-up

killdata_clean %>%
  group_by(signs_of_mental_illness) %>%
  summarise(count = n()) # Roughly 25% of victims had signs of mental illness (Y or N)

killdata_clean %>%
  group_by(threat_level) %>%
  summarise(count = n()) # Three categories, 'attack', 'other' and 'undetermined' (low freq)

killdata_clean %>%
  group_by(flee) %>%
  summarise(count = n()) # Five options: a blank, 'Car', 'Foot', 'Not fleeing' and 'Other'; potentially roll-up to fleeing and not fleeing

killdata_clean %>%
  group_by(body_camera) %>%
  summarise(count = n()) # Options FALSE or TRUE. Only ~14% of deaths had body cameras

# Given that there is not much data is it is -- it may not make sense to differentiate different types of killings (e.g. those where there was a threat or signs of mental illness). It may not make much sense to differentiate by year either. 
```

```{r}
# Curious to see how the killing is disseminated across counties represented in the data. Below is code to construct a pareto chart of killings by FIPS. 

killdata_FIPS <- killdata_clean %>% # Create a county-level data-set with # of killings
  group_by(mod_fips) %>%
  summarise(totalKillings = n()) %>%
  rename(fips = mod_fips)

paretoData <- arrange(killdata_FIPS, desc(totalKillings)) %>%
  mutate(cumsum = cumsum(totalKillings),
         freq = round(totalKillings/sum(totalKillings), 3), 
         cum_freq = cumsum(freq))

## Saving Parameters 
def_par <- par()

# New margins
par(mar=c(5,5,4,5))

## plot bars, pc will hold x values for bars
pc = barplot(paretoData$totalKillings,  
             width = 1, space = 0.2, border = NA, axes = F,
             ylim = c(0, 1.05 * max(paretoData$totalKillings, na.rm = T)), 
             ylab = "Cumulative Counts" , cex.names = 0.7, 
             names.arg = paretoData$fips,
             main = "Pareto Chart (version 2)")

## anotate left axis
axis(side = 2, at = c(0, paretoData$totalKillings), las = 1, col.axis = "grey62", col = "grey62", tick = T, cex.axis = 0.8)

## frame plot
box( col = "grey62")

## Cumulative Frequency Lines 
lines(pc, paretoData$cumsum, type = "b", cex = 0.7, pch = 19, col="cyan4")

## Annotate Right Axis
px <- paretoData$cum_freq * max(paretoData$totalKillings, na.rm = T)
lines(pc, px, type = "b", cex = 0.7, pch = 19, col="cyan4")

## restoring default parameter
axis(side = 4, at = c(0, px), labels = paste(c(0, round(paretoData$cum_freq * 100)) ,"%",sep=""), 
     las = 1, col.axis = "grey62", col = "cyan4", cex.axis = 0.8, col.axis = "cyan4")

# One FIPs has a staggeringly high killing number (255), but we have to scale this by population. There is a very long right tail and from the pareto shape it does seem like certain counties are over-represented.

```

```{r}
# The maximum number of total killings is in Los Angeles County at 255. 
killdata_FIPS$fips[which.max(killdata_FIPS$totalKillings)]
killdata_FIPS$totalKillings[which.max(killdata_FIPS$totalKillings)]
```

```{r}

# install.packages('skimr')
library(skimr)
# summary(killdata_clean)

# Evaluate data across time
killdata_fips_year <- killdata_clean %>%
  mutate(date = as.Date(killdata_clean$date, format = "%m/%d/%Y")) %>%
  mutate(year = lubridate::year(date)) %>%
  group_by(year) %>%
  summarise(totalKillings = n())

# There is not much difference year over year; might not make sense to separate this by years

# Pull out the FIPS code out of the ID identified in the census data
masterCensus_s2$fips <- str_sub(masterCensus_s2$id, -5, -1)

# Join the killings data to the census data. Create the killing rate response variable as well as transformed version because the rates are very low. 
sdl1 <- masterCensus_s2 %>%
    left_join(killdata_FIPS, by="fips") %>%
    mutate(killingRate =  totalKillings/Estimate..SEX.AND.AGE..Total.population) %>%
    mutate(killingRateEff = coalesce(killingRate, 0.000001)) %>%
    mutate(killingRateLog = log(killingRateEff)) %>%
    select(fips,totalKillings,killingRate,killingRateLog,everything())
```

```{r}
# We will do some variable/column cleaning

# Step 1 cleaning; only keep columns that have 'percentage' data. Countries differ greatly by size, so the nominal data could push results one way or another. By using percentage data, each variable in each county receives equal weight
sdl2 <- sdl1 %>%
    select(contains("Percent"))

# Step 2 cleaning; identify non-numeric values within census data
table(unlist(sdl2, use.names = FALSE)[(!grepl('^[0-9]',unlist(sdl2, use.names = FALSE)))])

# It looks like things are coded (X) or N if they are missing; let's look at which columns have a high percentage of (X) or N and are thus worth dropping

# Flag any column where more than half the values are '(X)'
library(plyr)
count.X.per.column <- ldply(sdl2, function(c) sum(c=="(X)"))
columnsToExclude1 <- count.X.per.column$.id[count.X.per.column$V1 > 420]

# Flag any column where more than half of the values are 'N'
count.N.per.column <- ldply(sdl2, function(c) sum(c=="N"))
columnsToExclude2 <- count.N.per.column$.id[count.N.per.column$V1 > 420]

# Drop columns with too many NULL values
sdl3 <- sdl2 %>%
  select(-columnsToExclude1)

sdl4 <- sdl3 %>%
  select(-columnsToExclude2)

# Convert remaining data frame into numeric columns (will force some values to NA)
sdl5 <- mutate_all(sdl4, function(x) as.numeric(as.character(x)))

# Flag any column where the minimum is above 100 (percentage cannot be over 100)
colMin <- apply(sdl5, 2, min, na.rm = TRUE)
columnsToExclude3 <- names(colMin)[colMin>100]

# Drop columns with values that are below the expected range 
sdl6 <- sdl5 %>%
  select(-columnsToExclude3)

# Flag any column where the maximum is above 100 (percentage cannot be over 100)
colMax <- apply(sdl6, 2, max, na.rm = TRUE)
columnsToExclude4 <- names(colMax)[colMax>100]

# Drop columns with value that exceeds the expected range
sdl7 <- sdl6 %>%
  select(-columnsToExclude4)

## NEW ADDITIONS ##
manualForceIn <- read.csv('census_columns_keep.csv.', header = FALSE)
manualForceIn[1,] <- "Estimate..SEX.AND.AGE..Total.population"

manualForceInCols <- intersect(names(sdl7), manualForceIn[,1])

sdl7 <- sdl7[, manualForceInCols]

# Re-add response variables back to cleaned census data
sdl7$fips <- sdl1$fips
sdl7$totalKillings <- sdl1$totalKillings
sdl7$killingRate <- sdl1$killingRate
sdl7$killingRateLog <- sdl1$killingRateLog

# Join on county and state variables onto data layer
sdl8 <- sdl7 %>%
  left_join(fipsLookUp, by="fips") %>%
  select(fips,County,State,totalKillings,killingRate,killingRateLog,everything()) %>%
  select(-ï..FIPS)
  
# options(max.print=400)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


Some questions to explore:
- In what situations is a suspect more likely to be shot by the police?
- Are body cameras associated with lower killings per capita?
- Is there a higher proportion of certain races shot compared to the population?
    - Does this vary by county?

Output variable: Killing per capita
Output variable: % black killings / % black population
    - Does average income affect this?
    - Does average education affect this?

Output variable: % of killings involving use of taser
- What factors contribute to greater use of taser?
  - More likely to be used against certain races?
  - More likely to be used when no weapon, or non-long range weapon carried by suspect?
  - When fleeing?


```{r}
# head(sdl1)
# turns all NAs into 0
# sdl2 <- sdl1 %>%
#   mutate_all(~replace(., is.na(.), 0)) %>%
#  relocate(totalKillings, .before = fips) %>%
#  relocate(killingRate, .before = fips) %>%
#  relocate(killingRateLog, .before = fips)
#   is.na(sdl2$killingRateLog)

# Remove non-predictive variables
sdl8sub = sdl8[,-1:-5]

# Glmnet doesn't play with NAs, so coalesce all NAs into 0
sdl9sub <- sdl8sub %>%
   mutate_all(~replace(., is.na(.), 0)) 

set.seed(1)

# Create model matrix for GLM
X.fl <- model.matrix(killingRateLog~., data=sdl9sub)[,-1]
Y <- sdl9sub$killingRateLog

# Run CV glmnet with ten-folds
fit.lasso.0 <- glmnet::cv.glmnet(X.fl,Y, nfolds = 10, intercept =T)
plot(fit.lasso.0)

# Identify variables associated with MSE at 1SE above minimum
coef.1se <- coef(fit.lasso.0, s="lambda.1se")
coef.1se <- coef.1se[which(coef.1se !=0),]
var.1se <- rownames(as.matrix(coef.1se))[-1]

# Extract selected variables out of data-set
data.fl.sub <- sdl9sub[,c("killingRateLog", var.1se)] # get a 

# Use selected variables in Relaxed-LASSO
fit.min.lm <- lm(killingRateLog~., data=data.fl.sub)

install.packages('gtsummary')
library(gtsummary)

# Get summary of regression results
fit.min.lm %>%
    tbl_regression()
```

```{r}

